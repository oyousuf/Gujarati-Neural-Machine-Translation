{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d3e19bb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import gzip\n",
    "import re\n",
    "import pandas as pd \n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74128fe4",
   "metadata": {
    "tags": []
   },
   "source": [
    "## DATA LOAD FOR TRAINING - from WMT 2019 task\n",
    "\n",
    "- __Train sets__\n",
    "1. Gujarati-English\n",
    "2. Hindi-English - large corpus "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ff3b22d",
   "metadata": {},
   "source": [
    "### - loading Training set "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "380d695d",
   "metadata": {},
   "source": [
    "- 5 tsv.gz files\n",
    "- __Additional corpus__ : gu-en parallel corpus that contains 65K sentences by Uka Tarsadia University, Gujarat, India "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b9aa1f12",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wk789\\Miniconda3\\envs\\gujarati_project\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3441: FutureWarning: The error_bad_lines argument has been deprecated and will be removed in a future version.\n",
      "\n",
      "\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
     ]
    }
   ],
   "source": [
    "parallel_data = []\n",
    "FILE_PATH = './dataset/train/'\n",
    "\n",
    "#extracting and loading Five tsv.gz files  into the dataframe\n",
    "file_list = glob.glob(FILE_PATH +'*.tsv.gz')\n",
    "for filename in file_list:\n",
    "    gujarat_eng = pd.read_csv(filename, delimiter='\\t', header=None, error_bad_lines=False, names=['source', 'target'])\n",
    "    parallel_data.append(gujarat_eng)\n",
    "\n",
    "#gujarati additional corpus \n",
    "with open(FILE_PATH + 'train.en.txt') as f:\n",
    "    en = f.read().split('\\n')\n",
    "    \n",
    "with open(FILE_PATH + 'train.gu.txt') as f:\n",
    "    gu = f.read().split('\\n')\n",
    "\n",
    "corp =  pd.DataFrame({'source': gu, 'target':en})   \n",
    "frame = pd.concat(parallel_data, axis=0, ignore_index=True)\n",
    "corpus = [corp, frame]\n",
    "corpus = pd.concat(corpus, axis=0)\n",
    "\n",
    "# adding column that marks the original language for future transliteration to Hindi\n",
    "corpus['origin_lang'] = 'gujarati'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "68e66024",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source</th>\n",
       "      <th>target</th>\n",
       "      <th>origin_lang</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ફ્રન્ટ વ્હીલ તરીકે ઘડિયાળ સાથે સાયકલ પ્રતિકૃતિ.</td>\n",
       "      <td>A bicycle replica with a clock as the front wh...</td>\n",
       "      <td>gujarati</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ગેરેજની સામે પાર્ક કરેલી બ્લેક હોન્ડા મોટરસાયકલ</td>\n",
       "      <td>A black Honda motorcycle parked in front of a ...</td>\n",
       "      <td>gujarati</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>વાદળી દિવાલો અને સફેદ સિંક અને બારણું ધરાવતી ખંડ</td>\n",
       "      <td>A room with blue walls and a white sink and door.</td>\n",
       "      <td>gujarati</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>એક કાર કે જે કાનૂની રીતે પાર્ક કરેલી કારની પાછ...</td>\n",
       "      <td>A car that seems to be parked illegally behind...</td>\n",
       "      <td>gujarati</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>હવામાં ઉડતી મોટી પેસેન્જર વિમાન.</td>\n",
       "      <td>A large passenger airplane flying through the ...</td>\n",
       "      <td>gujarati</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              source  \\\n",
       "0    ફ્રન્ટ વ્હીલ તરીકે ઘડિયાળ સાથે સાયકલ પ્રતિકૃતિ.   \n",
       "1    ગેરેજની સામે પાર્ક કરેલી બ્લેક હોન્ડા મોટરસાયકલ   \n",
       "2   વાદળી દિવાલો અને સફેદ સિંક અને બારણું ધરાવતી ખંડ   \n",
       "3  એક કાર કે જે કાનૂની રીતે પાર્ક કરેલી કારની પાછ...   \n",
       "4                   હવામાં ઉડતી મોટી પેસેન્જર વિમાન.   \n",
       "\n",
       "                                              target origin_lang  \n",
       "0  A bicycle replica with a clock as the front wh...    gujarati  \n",
       "1  A black Honda motorcycle parked in front of a ...    gujarati  \n",
       "2  A room with blue walls and a white sink and door.    gujarati  \n",
       "3  A car that seems to be parked illegally behind...    gujarati  \n",
       "4  A large passenger airplane flying through the ...    gujarati  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3388972b",
   "metadata": {},
   "source": [
    "- __Hindi-English corpus__ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "db6d94ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import codecs, string \n",
    "\n",
    "with open(FILE_PATH + '/training.txt') as f:\n",
    "    hin_en = f.read().split('\\n')\n",
    "    \n",
    "hin_en_parallel = []\n",
    "\n",
    "for sent in hin_en:\n",
    "    try:\n",
    "        hin, en = sent.split('\\t')\n",
    "        a = (hin, en)\n",
    "        if u'\\u0900' <= hin <= u'\\u097f': # to check if the sentence is in Hindi\n",
    "            if len(a) == 2 and a[0] != 0 and a[1]!=0 : #to get rid of dirty data that contains other languages or empty strings\n",
    "                hin_en_parallel.append(a)\n",
    "        \n",
    "        \n",
    "    except ValueError: \n",
    "        pass\n",
    "\n",
    "# to dataframe\n",
    "hin_en_df = pd.DataFrame(hin_en_parallel, columns = ['source', 'target'])\n",
    "hin_en_df['origin_lang'] = 'hindi' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "569f13ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source</th>\n",
       "      <th>target</th>\n",
       "      <th>origin_lang</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>सदस्य hi-2</td>\n",
       "      <td>User hi-2</td>\n",
       "      <td>hindi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>राष्ट्रीय विज्ञान दिवस</td>\n",
       "      <td>National Science Day</td>\n",
       "      <td>hindi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>अपने अनुप्रयोग को पहुंचनीयता व्यायाम का लाभ दें</td>\n",
       "      <td>Give your application an accessibility workout</td>\n",
       "      <td>hindi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>एक्सेर्साइसर पहुंचनीयता अन्वेषक</td>\n",
       "      <td>Accerciser Accessibility Explorer</td>\n",
       "      <td>hindi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>निचले पटल के लिए डिफोल्ट प्लग-इन खाका</td>\n",
       "      <td>The default plugin layout for the bottom panel</td>\n",
       "      <td>hindi</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            source  \\\n",
       "0                                       सदस्य hi-2   \n",
       "1                           राष्ट्रीय विज्ञान दिवस   \n",
       "2  अपने अनुप्रयोग को पहुंचनीयता व्यायाम का लाभ दें   \n",
       "3                  एक्सेर्साइसर पहुंचनीयता अन्वेषक   \n",
       "4            निचले पटल के लिए डिफोल्ट प्लग-इन खाका   \n",
       "\n",
       "                                           target origin_lang  \n",
       "0                                       User hi-2       hindi  \n",
       "1                            National Science Day       hindi  \n",
       "2  Give your application an accessibility workout       hindi  \n",
       "3               Accerciser Accessibility Explorer       hindi  \n",
       "4  The default plugin layout for the bottom panel       hindi  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hin_en_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7172f27b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gujarati-English + Hindi-English corpus \n",
    "corpus = [corpus, hin_en_df]\n",
    "corpus = pd.concat(corpus, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "99e3ad6a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source</th>\n",
       "      <th>target</th>\n",
       "      <th>origin_lang</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ફ્રન્ટ વ્હીલ તરીકે ઘડિયાળ સાથે સાયકલ પ્રતિકૃતિ.</td>\n",
       "      <td>A bicycle replica with a clock as the front wh...</td>\n",
       "      <td>gujarati</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ગેરેજની સામે પાર્ક કરેલી બ્લેક હોન્ડા મોટરસાયકલ</td>\n",
       "      <td>A black Honda motorcycle parked in front of a ...</td>\n",
       "      <td>gujarati</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>વાદળી દિવાલો અને સફેદ સિંક અને બારણું ધરાવતી ખંડ</td>\n",
       "      <td>A room with blue walls and a white sink and door.</td>\n",
       "      <td>gujarati</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>એક કાર કે જે કાનૂની રીતે પાર્ક કરેલી કારની પાછ...</td>\n",
       "      <td>A car that seems to be parked illegally behind...</td>\n",
       "      <td>gujarati</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>હવામાં ઉડતી મોટી પેસેન્જર વિમાન.</td>\n",
       "      <td>A large passenger airplane flying through the ...</td>\n",
       "      <td>gujarati</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1725228</th>\n",
       "      <td>कार्यक्रम को इनके जरिए लाइव स्ट्रीम किया जाएगा:</td>\n",
       "      <td>The programme will be streamed live via:</td>\n",
       "      <td>hindi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1725229</th>\n",
       "      <td>मानव संसाधन विकास मंत्रालय का फेसबुक पेज: http...</td>\n",
       "      <td>Ministry of Education Facebook Page: https://w...</td>\n",
       "      <td>hindi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1725230</th>\n",
       "      <td>यूजीसी यूट्यूब चैनल, पीआईबी यूट्यूब चैनल,</td>\n",
       "      <td>UGC YouTube Channel, PIB YouTube Channel,</td>\n",
       "      <td>hindi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1725231</th>\n",
       "      <td>यूजीसी ट्विटर हैंडल (@ugc_india) : https://twi...</td>\n",
       "      <td>UGC Twitter Handle (@ugc_india) : https://twit...</td>\n",
       "      <td>hindi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1725232</th>\n",
       "      <td>कार्यक्रम को डीडी न्यूज पर भी प्रसारित किया जा...</td>\n",
       "      <td>It would also be broadcast on DD News.</td>\n",
       "      <td>hindi</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1725233 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    source  \\\n",
       "0          ફ્રન્ટ વ્હીલ તરીકે ઘડિયાળ સાથે સાયકલ પ્રતિકૃતિ.   \n",
       "1          ગેરેજની સામે પાર્ક કરેલી બ્લેક હોન્ડા મોટરસાયકલ   \n",
       "2         વાદળી દિવાલો અને સફેદ સિંક અને બારણું ધરાવતી ખંડ   \n",
       "3        એક કાર કે જે કાનૂની રીતે પાર્ક કરેલી કારની પાછ...   \n",
       "4                         હવામાં ઉડતી મોટી પેસેન્જર વિમાન.   \n",
       "...                                                    ...   \n",
       "1725228    कार्यक्रम को इनके जरिए लाइव स्ट्रीम किया जाएगा:   \n",
       "1725229  मानव संसाधन विकास मंत्रालय का फेसबुक पेज: http...   \n",
       "1725230          यूजीसी यूट्यूब चैनल, पीआईबी यूट्यूब चैनल,   \n",
       "1725231  यूजीसी ट्विटर हैंडल (@ugc_india) : https://twi...   \n",
       "1725232  कार्यक्रम को डीडी न्यूज पर भी प्रसारित किया जा...   \n",
       "\n",
       "                                                    target origin_lang  \n",
       "0        A bicycle replica with a clock as the front wh...    gujarati  \n",
       "1        A black Honda motorcycle parked in front of a ...    gujarati  \n",
       "2        A room with blue walls and a white sink and door.    gujarati  \n",
       "3        A car that seems to be parked illegally behind...    gujarati  \n",
       "4        A large passenger airplane flying through the ...    gujarati  \n",
       "...                                                    ...         ...  \n",
       "1725228           The programme will be streamed live via:       hindi  \n",
       "1725229  Ministry of Education Facebook Page: https://w...       hindi  \n",
       "1725230          UGC YouTube Channel, PIB YouTube Channel,       hindi  \n",
       "1725231  UGC Twitter Handle (@ugc_india) : https://twit...       hindi  \n",
       "1725232             It would also be broadcast on DD News.       hindi  \n",
       "\n",
       "[1725233 rows x 3 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cb4147e",
   "metadata": {},
   "source": [
    "### - Checking the features of the corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1fa54482",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The size of the 5 Gujarati-English parallel corpus: 1725233\n",
      "The size of the Hindi-English parallel corpus: 1504696\n",
      "The total size of the parallel corpus: 3229929\n"
     ]
    }
   ],
   "source": [
    "print(f'The size of the 5 Gujarati-English parallel corpus: {len(corpus)}')\n",
    "print(f'The size of the Hindi-English parallel corpus: {len(hin_en_df)}')\n",
    "print(f'The total size of the parallel corpus: {len(hin_en_df)+len(corpus)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ecc1ddc",
   "metadata": {},
   "source": [
    "## 2. Preprocessing/Denoising corpus\n",
    "\n",
    "1. Deletion \n",
    "- NULL data\n",
    "- foreign character ratio over 50% for better performance \n",
    "- special characters in the sentences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "62f17d86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# General deletion for both languages \n",
    "def null_preprocess(corpus):\n",
    "    corpus.replace('', np.nan, inplace=True)\n",
    "    corpus.dropna(inplace=True)\n",
    "    \n",
    "    return corpus\n",
    "\n",
    "def character_ratio(data):\n",
    "    len_string = len(data)\n",
    "    len_non_latin = len(\"\".join(re.findall(r'[^a-zA-Z\\s]+', data)))\n",
    "    ratio = len_non_latin/len_string\n",
    "    \n",
    "    if ratio > 0.5:\n",
    "        return True \n",
    "    \n",
    "    else:\n",
    "        return False \n",
    "\n",
    "# method for target language preprocessing - English\n",
    "def eng_preprocess(corpus):\n",
    "    print(f'The size of corpus before preprocessing: {len(corpus)}')\n",
    "    \n",
    "    #null deletion \n",
    "    corpus = null_preprocess(corpus)\n",
    "    print(f'The size of corpus after preprocessing (null deletion): {len(corpus)}')\n",
    "\n",
    "    # special character deletion\n",
    "    corpus['target'] = corpus['target'].str.replace(pat=r'[^\\w]', repl=r' ', regex=True)\n",
    "    corpus['target'] = corpus['target'].str.replace(pat=r'\\s{2,}', repl=r' ', regex=True)\n",
    "    \n",
    "    #foreign char ratio deletion (to get rid of the english sentences that contain a lot of foreign characters)\n",
    "    idx_foreign = corpus[(corpus['target'].apply(character_ratio) == True)].index  \n",
    "    corpus = corpus.drop(idx_foreign)\n",
    "    print(f'The size of corpus after preprocessing (foreign character ratio deletion): {len(corpus)}')\n",
    "    \n",
    "    #delete row sentence length over 120\n",
    "    corpus['target_sent_len'] = [len(text) for text in corpus.target]\n",
    "    idx_sent_over_120 = corpus[(corpus['target_sent_len'] > 120)].index\n",
    "    corpus = corpus.drop(idx_sent_over_120)\n",
    "    print(f'The size of corpus after preprocessing (sentence length over 120 deletion): {len(corpus)}')\n",
    "\n",
    "    return corpus\n",
    "\n",
    "\n",
    "# method for source language preprocessing - Gujarati\n",
    "def guj_preprocess(corpus):\n",
    "\n",
    "    from indicnlp.normalize.indic_normalize import IndicNormalizerFactory\n",
    "    from indicnlp.tokenize import indic_tokenize \n",
    "    from indicnlp.transliterate.unicode_transliterate import UnicodeIndicTransliterator\n",
    "\n",
    "    #remove_nuktas=False\n",
    "    factory = IndicNormalizerFactory()\n",
    "    gu_normalizer = factory.get_normalizer(\"gu\", remove_nuktas=False)\n",
    "    hi_normalizer = factory.get_normalizer(\"hi\", remove_nuktas=False)\n",
    "    \n",
    "    corpus['source_normalized'] = np.where(corpus['origin_lang']=='gujarati', corpus.source.apply(lambda x: gu_normalizer.normalize(x)), corpus.source.apply(lambda x: hi_normalizer.normalize(x)))\n",
    "    \n",
    "    corpus['source_normalized_sent_len'] = [len(text) for text in corpus.source_normalized]\n",
    "    \n",
    "    #tokenize\n",
    "    \n",
    "    corpus['source_tokenized'] = np.where(corpus['origin_lang']=='gujarati', corpus.source_normalized.apply(lambda x: indic_tokenize.trivial_tokenize(x, lang='gu')), corpus.source_normalized.apply(lambda x: indic_tokenize.trivial_tokenize(x, lang='hi')))\n",
    "    corpus['source_tokenized'] = corpus.source_tokenized.apply((lambda x: \" \".join(x)))\n",
    "    \n",
    "    #script conversion\n",
    "    corpus['guja_2_hindi'] = np.where(corpus['origin_lang']=='gujarati', \n",
    "                                      corpus.source_tokenized.apply(lambda x: UnicodeIndicTransliterator.transliterate(x, 'gu', 'hi')), \n",
    "                                      corpus.source_tokenized.apply(lambda x : x))\n",
    "\n",
    "    return corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c740dbfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The size of corpus before preprocessing: 1725233\n",
      "The size of corpus after preprocessing (null deletion): 1725194\n",
      "The size of corpus after preprocessing (foreign character ratio deletion): 1723261\n",
      "The size of corpus after preprocessing (sentence length over 120 deletion): 1421614\n"
     ]
    }
   ],
   "source": [
    "corpus = eng_preprocess(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fd8d3bd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = guj_preprocess(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0d30babf-a76c-4ed0-8b01-443d46ee74bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source</th>\n",
       "      <th>target</th>\n",
       "      <th>origin_lang</th>\n",
       "      <th>target_sent_len</th>\n",
       "      <th>source_normalized</th>\n",
       "      <th>source_normalized_sent_len</th>\n",
       "      <th>source_tokenized</th>\n",
       "      <th>guja_2_hindi</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ફ્રન્ટ વ્હીલ તરીકે ઘડિયાળ સાથે સાયકલ પ્રતિકૃતિ.</td>\n",
       "      <td>A bicycle replica with a clock as the front wh...</td>\n",
       "      <td>gujarati</td>\n",
       "      <td>50</td>\n",
       "      <td>ફ્રન્ટ વ્હીલ તરીકે ઘડિયાળ સાથે સાયકલ પ્રતિકૃતિ.</td>\n",
       "      <td>47</td>\n",
       "      <td>ફ્રન્ટ વ્હીલ તરીકે ઘડિયાળ સાથે સાયકલ પ્રતિકૃતિ .</td>\n",
       "      <td>फ्रन्ट व्हील तरीके घडियाळ साथे सायकल प्रतिकृति .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ગેરેજની સામે પાર્ક કરેલી બ્લેક હોન્ડા મોટરસાયકલ</td>\n",
       "      <td>A black Honda motorcycle parked in front of a ...</td>\n",
       "      <td>gujarati</td>\n",
       "      <td>53</td>\n",
       "      <td>ગેરેજની સામે પાર્ક કરેલી બ્લેક હોન્ડા મોટરસાયકલ</td>\n",
       "      <td>47</td>\n",
       "      <td>ગેરેજની સામે પાર્ક કરેલી બ્લેક હોન્ડા મોટરસાયકલ</td>\n",
       "      <td>गेरेजनी सामे पार्क करेली ब्लेक होन्डा मोटरसायकल</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>વાદળી દિવાલો અને સફેદ સિંક અને બારણું ધરાવતી ખંડ</td>\n",
       "      <td>A room with blue walls and a white sink and door</td>\n",
       "      <td>gujarati</td>\n",
       "      <td>49</td>\n",
       "      <td>વાદળી દિવાલો અને સફેદ સિંક અને બારણું ધરાવતી ખંડ</td>\n",
       "      <td>48</td>\n",
       "      <td>વાદળી દિવાલો અને સફેદ સિંક અને બારણું ધરાવતી ખંડ</td>\n",
       "      <td>वादळी दिवालो अने सफेद सिंक अने बारणुं धरावती खंड</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>એક કાર કે જે કાનૂની રીતે પાર્ક કરેલી કારની પાછ...</td>\n",
       "      <td>A car that seems to be parked illegally behind...</td>\n",
       "      <td>gujarati</td>\n",
       "      <td>67</td>\n",
       "      <td>એક કાર કે જે કાનૂની રીતે પાર્ક કરેલી કારની પાછ...</td>\n",
       "      <td>95</td>\n",
       "      <td>એક કાર કે જે કાનૂની રીતે પાર્ક કરેલી કારની પાછ...</td>\n",
       "      <td>एक कार के जे कानूनी रीते पार्क करेली कारनी पाछ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>હવામાં ઉડતી મોટી પેસેન્જર વિમાન.</td>\n",
       "      <td>A large passenger airplane flying through the ...</td>\n",
       "      <td>gujarati</td>\n",
       "      <td>50</td>\n",
       "      <td>હવામાં ઉડતી મોટી પેસેન્જર વિમાન.</td>\n",
       "      <td>32</td>\n",
       "      <td>હવામાં ઉડતી મોટી પેસેન્જર વિમાન .</td>\n",
       "      <td>हवामां उडती मोटी पेसेन्जर विमान .</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              source  \\\n",
       "0    ફ્રન્ટ વ્હીલ તરીકે ઘડિયાળ સાથે સાયકલ પ્રતિકૃતિ.   \n",
       "1    ગેરેજની સામે પાર્ક કરેલી બ્લેક હોન્ડા મોટરસાયકલ   \n",
       "2   વાદળી દિવાલો અને સફેદ સિંક અને બારણું ધરાવતી ખંડ   \n",
       "3  એક કાર કે જે કાનૂની રીતે પાર્ક કરેલી કારની પાછ...   \n",
       "4                   હવામાં ઉડતી મોટી પેસેન્જર વિમાન.   \n",
       "\n",
       "                                              target origin_lang  \\\n",
       "0  A bicycle replica with a clock as the front wh...    gujarati   \n",
       "1  A black Honda motorcycle parked in front of a ...    gujarati   \n",
       "2  A room with blue walls and a white sink and door     gujarati   \n",
       "3  A car that seems to be parked illegally behind...    gujarati   \n",
       "4  A large passenger airplane flying through the ...    gujarati   \n",
       "\n",
       "   target_sent_len                                  source_normalized  \\\n",
       "0               50    ફ્રન્ટ વ્હીલ તરીકે ઘડિયાળ સાથે સાયકલ પ્રતિકૃતિ.   \n",
       "1               53    ગેરેજની સામે પાર્ક કરેલી બ્લેક હોન્ડા મોટરસાયકલ   \n",
       "2               49   વાદળી દિવાલો અને સફેદ સિંક અને બારણું ધરાવતી ખંડ   \n",
       "3               67  એક કાર કે જે કાનૂની રીતે પાર્ક કરેલી કારની પાછ...   \n",
       "4               50                   હવામાં ઉડતી મોટી પેસેન્જર વિમાન.   \n",
       "\n",
       "   source_normalized_sent_len  \\\n",
       "0                          47   \n",
       "1                          47   \n",
       "2                          48   \n",
       "3                          95   \n",
       "4                          32   \n",
       "\n",
       "                                    source_tokenized  \\\n",
       "0   ફ્રન્ટ વ્હીલ તરીકે ઘડિયાળ સાથે સાયકલ પ્રતિકૃતિ .   \n",
       "1    ગેરેજની સામે પાર્ક કરેલી બ્લેક હોન્ડા મોટરસાયકલ   \n",
       "2   વાદળી દિવાલો અને સફેદ સિંક અને બારણું ધરાવતી ખંડ   \n",
       "3  એક કાર કે જે કાનૂની રીતે પાર્ક કરેલી કારની પાછ...   \n",
       "4                  હવામાં ઉડતી મોટી પેસેન્જર વિમાન .   \n",
       "\n",
       "                                        guja_2_hindi  \n",
       "0   फ्रन्ट व्हील तरीके घडियाळ साथे सायकल प्रतिकृति .  \n",
       "1    गेरेजनी सामे पार्क करेली ब्लेक होन्डा मोटरसायकल  \n",
       "2   वादळी दिवालो अने सफेद सिंक अने बारणुं धरावती खंड  \n",
       "3  एक कार के जे कानूनी रीते पार्क करेली कारनी पाछ...  \n",
       "4                  हवामां उडती मोटी पेसेन्जर विमान .  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9788f815",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "વાદળી દિવાલો અને સફેદ સિંક અને બારણું ધરાવતી ખંડ \t वादळी दिवालो अने सफेद सिंक अने बारणुं धरावती खंड\n"
     ]
    }
   ],
   "source": [
    "# Checking the transliteration from Gujarati to Hindi\n",
    "print(corpus.source[2],'\\t' ,corpus.guja_2_hindi[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "186c9508",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 3. Subword level tokenization using Sentencepiece -(BPE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcc9a01e",
   "metadata": {},
   "source": [
    "__1. Baseline__\n",
    "- Based on `Hindi-English` corpus\n",
    "\n",
    "__2. Multilingual__ \n",
    "- Based on `Hindi-English` + `Gujarati - English` (translated to Hindi script) corpus "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b0acd58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataframe to store hindi-english model   \n",
    "baseline = pd.DataFrame()\n",
    "multilingual = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13325282",
   "metadata": {},
   "outputs": [],
   "source": [
    "hi = corpus_prac.source[(corpus_prac.origin_lang=='hindi')]\n",
    "\n",
    "with open('./dataset/train/sentencepiece/hi/hi_train.txt', 'w', encoding='utf-8') as f:\n",
    "    for line in hi:\n",
    "        f.write(line+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "442cf578",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sentencepiece as spm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0e082518",
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline = pd.DataFrame()\n",
    "multilingual = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13c3def4-e093-4577-9c98-70a19b58f96a",
   "metadata": {},
   "source": [
    "## Baseline\n",
    "### 3.1. Hindi subword tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "18703540",
   "metadata": {},
   "outputs": [],
   "source": [
    "SENT_FILE_PATH = './dataset/train/sentencepiece/'\n",
    "hi = corpus.source_tokenized[(corpus.origin_lang=='hindi')]\n",
    "with open(SENT_FILE_PATH + 'hi/hi_train.txt', 'w', encoding='utf-8') as f:\n",
    "    for line in hi:\n",
    "        f.write(line+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6f8ee9c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train done\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sentencepiece as spm\n",
    "\n",
    "input_file = SENT_FILE_PATH + 'hi/hi_train.txt'\n",
    "sp_model_root = SENT_FILE_PATH + 'hi/'\n",
    "\n",
    "if not os.path.isdir(sp_model_root):\n",
    "    os.mkdir(sp_model_root)\n",
    "    print('directory created.')\n",
    "    \n",
    "vocab_size = 32000    \n",
    "sp_model_name = f'tokenizer_{vocab_size}'\n",
    "sp_model_path = os.path.join(sp_model_root, sp_model_name)\n",
    "model_type = 'bpe'\n",
    "character_coverage = 1.0\n",
    "user_defined_symbols = '[PAD],[UNK],[CLS],[SEP],[MASK],[BOS],[EOS],[UNK0],[UNK1],[UNK2],[UNK3],[UNK4],[UNK5],[UNK6],[UNK7],[UNK8],[UNK9],[unused0],[unused1],[unused2],[unused3],[unused4],[unused5],[unused6],[unused7],[unused8],[unused9],[unused10],[unused11],[unused12],[unused13],[unused14],[unused15],[unused16],[unused17],[unused18],[unused19],[unused20],[unused21],[unused22],[unused23],[unused24],[unused25],[unused26],[unused27],[unused28],[unused29],[unused30],[unused31],[unused32],[unused33],[unused34],[unused35],[unused36],[unused37],[unused38],[unused39],[unused40],[unused41],[unused42],[unused43],[unused44],[unused45],[unused46],[unused47],[unused48],[unused49],[unused50],[unused51],[unused52],[unused53],[unused54],[unused55],[unused56],[unused57],[unused58],[unused59],[unused60],[unused61],[unused62],[unused63],[unused64],[unused65],[unused66],[unused67],[unused68],[unused69],[unused70],[unused71],[unused72],[unused73],[unused74],[unused75],[unused76],[unused77],[unused78],[unused79],[unused80],[unused81],[unused82],[unused83],[unused84],[unused85],[unused86],[unused87],[unused88],[unused89],[unused90],[unused91],[unused92],[unused93],[unused94],[unused95],[unused96],[unused97],[unused98],[unused99]'\n",
    "input_argument = '--input=%s --model_prefix=%s --vocab_size=%s --user_defined_symbols=%s --model_type=%s --character_coverage=%s'\n",
    "cmd = input_argument%(input_file, sp_model_path, vocab_size,user_defined_symbols, model_type, character_coverage)\n",
    "\n",
    "spm.SentencePieceTrainer.Train(cmd)\n",
    "print('train done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fc7d81d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "चुनाव को हटाएं ( C _ )\n",
      "['▁चुनाव', '▁को', '▁हटाएं', '▁(', '▁C', '▁_', '▁)']\n",
      "[3615, 157, 10529, 178, 1312, 355, 176]\n"
     ]
    }
   ],
   "source": [
    "sp = spm.SentencePieceProcessor()\n",
    "sp.Load(f'{sp_model_path}.model')\n",
    "tokens = sp.encode_as_pieces(hi[220560])\n",
    "ids = sp.encode_as_ids(hi[220560])\n",
    "print(hi[220560])\n",
    "print(tokens)\n",
    "print(ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "43143cf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline['source'] = [sp.encode_as_ids(text) for text in corpus.guja_2_hindi]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "66e76751",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[4429, 1395, 25708, 2698, 309, 2228, 31822, 34...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[164, 5150, 12829, 410, 31655, 5139, 943, 296,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[2494, 21488, 368, 26313, 1217, 3699, 11718, 1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[186, 261, 136, 1883, 3758, 2287, 187, 5139, 9...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[122, 31669, 3829, 31662, 18959, 206, 9504, 12...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              source\n",
       "0  [4429, 1395, 25708, 2698, 309, 2228, 31822, 34...\n",
       "1  [164, 5150, 12829, 410, 31655, 5139, 943, 296,...\n",
       "2  [2494, 21488, 368, 26313, 1217, 3699, 11718, 1...\n",
       "3  [186, 261, 136, 1883, 3758, 2287, 187, 5139, 9...\n",
       "4  [122, 31669, 3829, 31662, 18959, 206, 9504, 12..."
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "baseline.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33ac0f22",
   "metadata": {},
   "source": [
    "### 3.2. English subword tokenization (Hindi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1300611d",
   "metadata": {},
   "outputs": [],
   "source": [
    "en_hi = corpus.target[(corpus.origin_lang=='hindi')]\n",
    "\n",
    "with open(SENT_FILE_PATH + 'en_hi/en_hi_train.txt', 'w', encoding='utf-8') as f:\n",
    "    for line in en_hi:\n",
    "        f.write(line+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "436cbac8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train done\n"
     ]
    }
   ],
   "source": [
    "input_file = SENT_FILE_PATH + 'en_hi/en_hi_train.txt'\n",
    "sp_model_root = SENT_FILE_PATH + 'en_hi/'\n",
    "\n",
    "if not os.path.isdir(sp_model_root):\n",
    "    os.mkdir(sp_model_root)\n",
    "    print('directory created!')\n",
    "\n",
    "    \n",
    "vocab_size = 32000    \n",
    "sp_model_name = f'tokenizer_{vocab_size}'\n",
    "sp_model_path = os.path.join(sp_model_root, sp_model_name)\n",
    "model_type = 'bpe'\n",
    "character_coverage = 1.0\n",
    "user_defined_symbols = '[PAD],[UNK],[CLS],[SEP],[MASK],[BOS],[EOS],[UNK0],[UNK1],[UNK2],[UNK3],[UNK4],[UNK5],[UNK6],[UNK7],[UNK8],[UNK9],[unused0],[unused1],[unused2],[unused3],[unused4],[unused5],[unused6],[unused7],[unused8],[unused9],[unused10],[unused11],[unused12],[unused13],[unused14],[unused15],[unused16],[unused17],[unused18],[unused19],[unused20],[unused21],[unused22],[unused23],[unused24],[unused25],[unused26],[unused27],[unused28],[unused29],[unused30],[unused31],[unused32],[unused33],[unused34],[unused35],[unused36],[unused37],[unused38],[unused39],[unused40],[unused41],[unused42],[unused43],[unused44],[unused45],[unused46],[unused47],[unused48],[unused49],[unused50],[unused51],[unused52],[unused53],[unused54],[unused55],[unused56],[unused57],[unused58],[unused59],[unused60],[unused61],[unused62],[unused63],[unused64],[unused65],[unused66],[unused67],[unused68],[unused69],[unused70],[unused71],[unused72],[unused73],[unused74],[unused75],[unused76],[unused77],[unused78],[unused79],[unused80],[unused81],[unused82],[unused83],[unused84],[unused85],[unused86],[unused87],[unused88],[unused89],[unused90],[unused91],[unused92],[unused93],[unused94],[unused95],[unused96],[unused97],[unused98],[unused99]'\n",
    "input_argument = '--input=%s --model_prefix=%s --vocab_size=%s --user_defined_symbols=%s --model_type=%s --character_coverage=%s'\n",
    "cmd = input_argument%(input_file, sp_model_path, vocab_size,user_defined_symbols, model_type, character_coverage)\n",
    "\n",
    "spm.SentencePieceTrainer.Train(cmd)\n",
    "print('train done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "de511ba2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "National Science Day\n",
      "['▁National', '▁Science', '▁Day']\n",
      "[1274, 4407, 1023]\n"
     ]
    }
   ],
   "source": [
    "sp = spm.SentencePieceProcessor()\n",
    "sp.Load(f'{sp_model_path}.model')\n",
    "tokens = sp.encode_as_pieces(en_hi[220538])\n",
    "ids = sp.encode_as_ids(en_hi[220538])\n",
    "print(en_hi[220538])\n",
    "print(tokens)\n",
    "print(ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "990019a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline['target'] = [sp.encode_as_ids(text) for text in corpus.target]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96b96683",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 3.3 Gujarati + Hindi in Hindi script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a84e33cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(SENT_FILE_PATH + 'gu_hi/gu_hi_train.txt', 'w', encoding='utf-8') as f:\n",
    "    for line in corpus.guja_2_hindi:\n",
    "        f.write(line+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "599442d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "directory created!\n",
      "train done\n"
     ]
    }
   ],
   "source": [
    "input_file = SENT_FILE_PATH + 'gu_hi/gu_hi_train.txt'\n",
    "sp_model_root = 'gu_hi/'\n",
    "\n",
    "if not os.path.isdir(sp_model_root):\n",
    "    os.mkdir(sp_model_root)\n",
    "    print('directory created!')\n",
    "    \n",
    "vocab_size = 32000    \n",
    "sp_model_name = f'tokenizer_{vocab_size}'\n",
    "sp_model_path = os.path.join(sp_model_root, sp_model_name)\n",
    "model_type = 'bpe'\n",
    "character_coverage = 1.0\n",
    "user_defined_symbols = '[PAD],[UNK],[CLS],[SEP],[MASK],[BOS],[EOS],[UNK0],[UNK1],[UNK2],[UNK3],[UNK4],[UNK5],[UNK6],[UNK7],[UNK8],[UNK9],[unused0],[unused1],[unused2],[unused3],[unused4],[unused5],[unused6],[unused7],[unused8],[unused9],[unused10],[unused11],[unused12],[unused13],[unused14],[unused15],[unused16],[unused17],[unused18],[unused19],[unused20],[unused21],[unused22],[unused23],[unused24],[unused25],[unused26],[unused27],[unused28],[unused29],[unused30],[unused31],[unused32],[unused33],[unused34],[unused35],[unused36],[unused37],[unused38],[unused39],[unused40],[unused41],[unused42],[unused43],[unused44],[unused45],[unused46],[unused47],[unused48],[unused49],[unused50],[unused51],[unused52],[unused53],[unused54],[unused55],[unused56],[unused57],[unused58],[unused59],[unused60],[unused61],[unused62],[unused63],[unused64],[unused65],[unused66],[unused67],[unused68],[unused69],[unused70],[unused71],[unused72],[unused73],[unused74],[unused75],[unused76],[unused77],[unused78],[unused79],[unused80],[unused81],[unused82],[unused83],[unused84],[unused85],[unused86],[unused87],[unused88],[unused89],[unused90],[unused91],[unused92],[unused93],[unused94],[unused95],[unused96],[unused97],[unused98],[unused99]'\n",
    "input_argument = '--input=%s --model_prefix=%s --vocab_size=%s --user_defined_symbols=%s --model_type=%s --character_coverage=%s'\n",
    "cmd = input_argument%(input_file, sp_model_path, vocab_size,user_defined_symbols, model_type, character_coverage)\n",
    "\n",
    "spm.SentencePieceTrainer.Train(cmd)\n",
    "print('train done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5eb09923",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1850मां शहेरनी दरेक वाणिज्यिक अने आर्थिक बाबतो कपास साथे संबंधित हती .\n",
      "['▁18', '50', 'मां', '▁शहेरनी', '▁दरेक', '▁वाणिज्यिक', '▁अने', '▁आर्थिक', '▁बाबतो', '▁कपास', '▁साथे', '▁संबंधित', '▁हती', '▁.']\n",
      "[2068, 6314, 227, 3014, 4163, 6354, 318, 1804, 6564, 12094, 450, 1261, 2193, 150]\n"
     ]
    }
   ],
   "source": [
    "sp = spm.SentencePieceProcessor()\n",
    "sp.Load(f'{sp_model_path}.model')\n",
    "tokens = sp.encode_as_pieces(corpus.guja_2_hindi[200000])\n",
    "ids = sp.encode_as_ids(corpus.guja_2_hindi[200000])\n",
    "print(corpus.guja_2_hindi[200000])\n",
    "print(tokens)\n",
    "print(ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4b3cd7c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "multilingual['source'] = [sp.encode_as_ids(text) for text in corpus.guja_2_hindi]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "dfb81bbd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[12260, 11492, 1498, 4647, 450, 3874, 18543, 150]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[19598, 199, 1890, 1383, 4096, 8879, 175, 1538...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[2291, 8034, 318, 1057, 1797, 318, 6453, 4714,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[172, 253, 135, 564, 4348, 1650, 1383, 4096, 5...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[8630, 5665, 2364, 5476, 1499, 150]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1421609</th>\n",
       "      <td>[1086, 161, 3698, 6302, 19302, 5492, 262, 28719]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1421610</th>\n",
       "      <td>[1865, 2985, 853, 3066, 163, 19634, 6240, 3157...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1421611</th>\n",
       "      <td>[856, 341, 243, 21548, 4571, 152, 14499, 21548...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1421612</th>\n",
       "      <td>[856, 341, 243, 15088, 12314, 15944, 185, 1951...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1421613</th>\n",
       "      <td>[1086, 161, 26425, 16469, 168, 236, 12966, 262...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1421614 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    source\n",
       "0        [12260, 11492, 1498, 4647, 450, 3874, 18543, 150]\n",
       "1        [19598, 199, 1890, 1383, 4096, 8879, 175, 1538...\n",
       "2        [2291, 8034, 318, 1057, 1797, 318, 6453, 4714,...\n",
       "3        [172, 253, 135, 564, 4348, 1650, 1383, 4096, 5...\n",
       "4                      [8630, 5665, 2364, 5476, 1499, 150]\n",
       "...                                                    ...\n",
       "1421609   [1086, 161, 3698, 6302, 19302, 5492, 262, 28719]\n",
       "1421610  [1865, 2985, 853, 3066, 163, 19634, 6240, 3157...\n",
       "1421611  [856, 341, 243, 21548, 4571, 152, 14499, 21548...\n",
       "1421612  [856, 341, 243, 15088, 12314, 15944, 185, 1951...\n",
       "1421613  [1086, 161, 26425, 16469, 168, 236, 12966, 262...\n",
       "\n",
       "[1421614 rows x 1 columns]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multilingual"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afc6ae98",
   "metadata": {},
   "source": [
    "### 3.4 English subword tokenization (Hindi+Gujarati)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c8863a5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(SENT_FILE_PATH + 'en_gu_hi/en_gu_hi_train.txt', 'w', encoding='utf-8') as f:\n",
    "    for line in corpus.target:\n",
    "        f.write(line+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ba73cd3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train done\n"
     ]
    }
   ],
   "source": [
    "input_file = SENT_FILE_PATH + 'en_gu_hi/en_gu_hi_train.txt'\n",
    "sp_model_root = SENT_FILE_PATH + 'en_gu_hi/'\n",
    "if not os.path.isdir(sp_model_root):\n",
    "    os.mkdir(sp_model_root)\n",
    "    print('directory created!')\n",
    "\n",
    "    \n",
    "vocab_size = 32000    \n",
    "sp_model_name = f'tokenizer_{vocab_size}'\n",
    "sp_model_path = os.path.join(sp_model_root, sp_model_name)\n",
    "model_type = 'bpe'\n",
    "character_coverage = 1.0\n",
    "user_defined_symbols = '[PAD],[UNK],[CLS],[SEP],[MASK],[BOS],[EOS],[UNK0],[UNK1],[UNK2],[UNK3],[UNK4],[UNK5],[UNK6],[UNK7],[UNK8],[UNK9],[unused0],[unused1],[unused2],[unused3],[unused4],[unused5],[unused6],[unused7],[unused8],[unused9],[unused10],[unused11],[unused12],[unused13],[unused14],[unused15],[unused16],[unused17],[unused18],[unused19],[unused20],[unused21],[unused22],[unused23],[unused24],[unused25],[unused26],[unused27],[unused28],[unused29],[unused30],[unused31],[unused32],[unused33],[unused34],[unused35],[unused36],[unused37],[unused38],[unused39],[unused40],[unused41],[unused42],[unused43],[unused44],[unused45],[unused46],[unused47],[unused48],[unused49],[unused50],[unused51],[unused52],[unused53],[unused54],[unused55],[unused56],[unused57],[unused58],[unused59],[unused60],[unused61],[unused62],[unused63],[unused64],[unused65],[unused66],[unused67],[unused68],[unused69],[unused70],[unused71],[unused72],[unused73],[unused74],[unused75],[unused76],[unused77],[unused78],[unused79],[unused80],[unused81],[unused82],[unused83],[unused84],[unused85],[unused86],[unused87],[unused88],[unused89],[unused90],[unused91],[unused92],[unused93],[unused94],[unused95],[unused96],[unused97],[unused98],[unused99]'\n",
    "input_argument = '--input=%s --model_prefix=%s --vocab_size=%s --user_defined_symbols=%s --model_type=%s --character_coverage=%s'\n",
    "cmd = input_argument%(input_file, sp_model_path, vocab_size,user_defined_symbols, model_type, character_coverage)\n",
    "\n",
    "spm.SentencePieceTrainer.Train(cmd)\n",
    "print('train done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "80472d13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A black Honda motorcycle parked in front of a garage \n",
      "['▁A', '▁black', '▁H', 'onda', '▁motorcycle', '▁parked', '▁in', '▁front', '▁of', '▁a', '▁garage']\n",
      "[155, 1392, 207, 16673, 1734, 1443, 151, 1283, 143, 122, 13976]\n"
     ]
    }
   ],
   "source": [
    "sp = spm.SentencePieceProcessor()\n",
    "sp.Load(f'{sp_model_path}.model')\n",
    "tokens = sp.encode_as_pieces(corpus.target[1])\n",
    "ids = sp.encode_as_ids(corpus.target[1])\n",
    "print(corpus.target[1])\n",
    "print(tokens)\n",
    "print(ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "17835e99",
   "metadata": {},
   "outputs": [],
   "source": [
    "multilingual['target'] = [sp.encode_as_ids(text) for text in corpus.target]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ec5e858-eeaa-466c-aa94-205f34dde062",
   "metadata": {},
   "source": [
    "## TEST DATASET LOAD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "2d086fcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tgz extract \n",
    "\n",
    "import tarfile\n",
    "from bs4 import BeautifulSoup, SoupStrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "425aa6fe-c724-4115-9b16-8342cc61d454",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dev = tarfile.open('./dataset/dev.tgz', 'r:gz')\n",
    "# for item in dev:\n",
    "#     dev.extract(item, './dataset/')\n",
    "\n",
    "TEST_FILE_PATH = './dataset/test/'        \n",
    "test = tarfile.open(TEST_FILE_PATH + 'test.tgz', 'r:gz')\n",
    "for item in test:\n",
    "    test.extract(item, TEST_FILE_PATH)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
